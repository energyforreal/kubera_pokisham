{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ Trading Agent - ML Model Training Pipeline\n",
    "\n",
    "This notebook trains XGBoost models for BTCUSDT across multiple timeframes (15m, 1h, 4h) and packages them for download.\n",
    "\n",
    "## Sections:\n",
    "1. Environment Setup\n",
    "2. Project Setup\n",
    "3. Configuration\n",
    "4. Data Fetching & Preparation\n",
    "5. Model Training\n",
    "6. Model Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Setup\n",
    "Install system dependencies and Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"âœ“ Running in Google Colab\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"âœ— Not running in Google Colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Install TA-Lib system dependencies\n",
    "echo \"Installing TA-Lib...\"\n",
    "wget -q http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\n",
    "tar -xzf ta-lib-0.4.0-src.tar.gz\n",
    "cd ta-lib/\n",
    "./configure --prefix=/usr\n",
    "make\n",
    "sudo make install\n",
    "cd ..\n",
    "rm -rf ta-lib ta-lib-0.4.0-src.tar.gz\n",
    "echo \"âœ“ TA-Lib installed successfully\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Python packages\n",
    "!pip install -q pandas numpy scikit-learn xgboost joblib\n",
    "!pip install -q TA-Lib\n",
    "!pip install -q aiohttp requests python-dateutil pytz pyyaml structlog\n",
    "!pip install -q matplotlib seaborn plotly tqdm\n",
    "\n",
    "print(\"âœ“ All packages installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import zipfile\n",
    "import getpass\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "import talib\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Project Setup\n",
    "Upload project files and configure credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    import shutil\n",
    "    \n",
    "    print(\"ðŸ“ Upload your project as a ZIP file (should contain src/, config/ folders)\")\n",
    "    print(\"Or you can use git clone if your project is on GitHub\\n\")\n",
    "    \n",
    "    choice = input(\"Enter '1' to upload ZIP or '2' to clone from git: \").strip()\n",
    "    \n",
    "    if choice == '1':\n",
    "        uploaded = files.upload()\n",
    "        \n",
    "        # Extract the uploaded zip\n",
    "        for filename in uploaded.keys():\n",
    "            if filename.endswith('.zip'):\n",
    "                print(f\"\\nExtracting {filename}...\")\n",
    "                with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "                    zip_ref.extractall('.')\n",
    "                print(\"âœ“ Project extracted\")\n",
    "    else:\n",
    "        repo_url = input(\"Enter GitHub repository URL: \").strip()\n",
    "        \n",
    "        # Clone to temp directory\n",
    "        temp_dir = 'temp_project'\n",
    "        if os.path.exists(temp_dir):\n",
    "            shutil.rmtree(temp_dir)\n",
    "        \n",
    "        !git clone {repo_url} {temp_dir}\n",
    "        \n",
    "        # Move files properly using shutil (handles overwriting)\n",
    "        if os.path.exists(temp_dir):\n",
    "            print(\"\\nðŸ“¦ Moving project files...\")\n",
    "            for item in os.listdir(temp_dir):\n",
    "                if item.startswith('.'):  # Skip hidden files like .git\n",
    "                    continue\n",
    "                \n",
    "                src_path = os.path.join(temp_dir, item)\n",
    "                dst_path = item\n",
    "                \n",
    "                # Remove destination if exists\n",
    "                if os.path.exists(dst_path):\n",
    "                    if os.path.isdir(dst_path):\n",
    "                        shutil.rmtree(dst_path)\n",
    "                    else:\n",
    "                        os.remove(dst_path)\n",
    "                \n",
    "                # Move from temp to current directory\n",
    "                shutil.move(src_path, dst_path)\n",
    "                print(f\"  âœ… {item}\")\n",
    "            \n",
    "            # Clean up temp directory\n",
    "            shutil.rmtree(temp_dir)\n",
    "            print(\"\\nâœ“ Project cloned and files moved successfully\")\n",
    "else:\n",
    "    print(\"â„¹ Not in Colab - assuming project files are already present\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify project structure\n",
    "print(\"ðŸ” Verifying project structure...\\n\")\n",
    "\n",
    "# Check current directory contents\n",
    "print(\"ðŸ“‚ Current directory:\", os.getcwd())\n",
    "print(\"ðŸ“‹ Directory contents:\")\n",
    "for item in sorted(os.listdir('.')):\n",
    "    if not item.startswith('.'):\n",
    "        icon = \"ðŸ“\" if os.path.isdir(item) else \"ðŸ“„\"\n",
    "        print(f\"  {icon} {item}\")\n",
    "\n",
    "# Check required directories\n",
    "print(\"\\nâœ… Checking required directories:\")\n",
    "required_dirs = ['src', 'config']\n",
    "missing_dirs = []\n",
    "\n",
    "for req_dir in required_dirs:\n",
    "    if os.path.exists(req_dir):\n",
    "        print(f\"  âœ… {req_dir}/\")\n",
    "        # Show subdirectories\n",
    "        if os.path.isdir(req_dir):\n",
    "            subdirs = [d for d in os.listdir(req_dir) if os.path.isdir(os.path.join(req_dir, d)) and not d.startswith('.')]\n",
    "            for subdir in subdirs:\n",
    "                print(f\"     â””â”€â”€ {subdir}/\")\n",
    "    else:\n",
    "        print(f\"  âŒ {req_dir}/ (missing)\")\n",
    "        missing_dirs.append(req_dir)\n",
    "\n",
    "if missing_dirs:\n",
    "    print(f\"\\nâŒ ERROR: Missing directories: {missing_dirs}\")\n",
    "    print(\"\\nðŸ”„ SOLUTION:\")\n",
    "    print(\"1. Go back to the 'Project Upload' cell above\")\n",
    "    print(\"2. Re-run it and upload/clone your project again\")\n",
    "    print(\"3. Make sure your project has the correct structure\")\n",
    "    raise FileNotFoundError(f\"Missing required directories: {missing_dirs}\")\n",
    "else:\n",
    "    print(\"\\nâœ… Project structure verified!\")\n",
    "    \n",
    "# Add current directory to Python path\n",
    "if os.getcwd() not in sys.path:\n",
    "    sys.path.insert(0, os.getcwd())\n",
    "    print(f\"âœ… Added '{os.getcwd()}' to Python path\")\n",
    "\n",
    "# Verify key modules exist\n",
    "print(\"\\nðŸ” Checking key Python modules:\")\n",
    "key_modules = [\n",
    "    'src/data/delta_client.py',\n",
    "    'src/data/feature_engineer.py',\n",
    "    'src/data/data_validator.py',\n",
    "    'src/ml/xgboost_model.py',\n",
    "]\n",
    "\n",
    "all_modules_found = True\n",
    "for module_path in key_modules:\n",
    "    exists = os.path.exists(module_path)\n",
    "    status = \"âœ…\" if exists else \"âŒ\"\n",
    "    print(f\"  {status} {module_path}\")\n",
    "    if not exists:\n",
    "        all_modules_found = False\n",
    "\n",
    "if not all_modules_found:\n",
    "    print(\"\\nâš ï¸  Some modules are missing. Check your repository structure.\")\n",
    "    print(\"   Your project should have been cloned from GitHub.\")\n",
    "else:\n",
    "    print(\"\\nðŸŽ‰ All modules found! Ready to proceed.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create necessary directories\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "print(\"âœ“ Created directories: models/, logs/, plots/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Delta Exchange API credentials\n",
    "print(\"ðŸ” Delta Exchange API Configuration\\n\")\n",
    "\n",
    "# Try to get from Colab secrets first\n",
    "api_key = None\n",
    "api_secret = None\n",
    "\n",
    "if IN_COLAB:\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        api_key = userdata.get('DELTA_API_KEY')\n",
    "        api_secret = userdata.get('DELTA_API_SECRET')\n",
    "        print(\"âœ“ Loaded credentials from Colab secrets\")\n",
    "    except:\n",
    "        print(\"â„¹ Colab secrets not found, using manual input\")\n",
    "\n",
    "# Manual input if not found\n",
    "if not api_key:\n",
    "    api_key = getpass.getpass('Enter Delta API Key: ')\n",
    "if not api_secret:\n",
    "    api_secret = getpass.getpass('Enter Delta API Secret: ')\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['DELTA_API_KEY'] = api_key\n",
    "os.environ['DELTA_API_SECRET'] = api_secret\n",
    "os.environ['DELTA_API_URL'] = 'https://api.india.delta.exchange'\n",
    "\n",
    "print(\"\\nâœ“ API credentials configured\")\n",
    "print(\"ðŸ“ Using Delta Exchange India API\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Configuration\n",
    "Load configuration and define training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project modules\n",
    "from src.data.delta_client import DeltaExchangeClient\n",
    "from src.data.feature_engineer import FeatureEngineer\n",
    "from src.data.data_validator import DataValidator\n",
    "from src.ml.xgboost_model import XGBoostTradingModel\n",
    "from src.ml.trainer import ModelTrainer\n",
    "\n",
    "print(\"âœ“ Project modules imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "SYMBOL = 'BTCUSD'\n",
    "TIMEFRAMES = ['15m', '1h', '4h']\n",
    "HISTORICAL_DAYS = 365  # 1 year of data\n",
    "\n",
    "# Model parameters\n",
    "MODEL_PARAMS = {\n",
    "    'n_estimators': 200,\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': 3,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'use_label_encoder': False,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "# Training parameters\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "\n",
    "print(\"ðŸ“Š Training Configuration:\")\n",
    "print(f\"  Symbol: {SYMBOL}\")\n",
    "print(f\"  Timeframes: {TIMEFRAMES}\")\n",
    "print(f\"  Historical Data: {HISTORICAL_DAYS} days\")\n",
    "print(f\"  Train/Val/Test Split: {TRAIN_RATIO}/{VAL_RATIO}/{TEST_RATIO}\")\n",
    "print(f\"\\n  Model: XGBoost Classifier\")\n",
    "print(f\"  Estimators: {MODEL_PARAMS['n_estimators']}\")\n",
    "print(f\"  Max Depth: {MODEL_PARAMS['max_depth']}\")\n",
    "print(f\"  Learning Rate: {MODEL_PARAMS['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Data Fetching & Preparation\n",
    "Fetch historical data, validate, and create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize clients\n",
    "delta_client = DeltaExchangeClient()\n",
    "feature_engineer = FeatureEngineer()\n",
    "validator = DataValidator()\n",
    "\n",
    "print(\"âœ“ Initialized data pipeline components\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_prepare_data(symbol: str, timeframe: str, days: int = 365) -> pd.DataFrame:\n",
    "    \"\"\"Fetch and prepare training data for a specific timeframe.\"\"\"\n",
    "    print(f\"\\nðŸ“¥ Fetching data for {symbol} - {timeframe}...\")\n",
    "    \n",
    "    # Fetch historical data\n",
    "    end_date = datetime.utcnow()\n",
    "    start_date = end_date - timedelta(days=days)\n",
    "    \n",
    "    df = delta_client.get_ohlc_candles(\n",
    "        symbol=symbol,\n",
    "        resolution=timeframe,\n",
    "        start=start_date,\n",
    "        end=end_date,\n",
    "        limit=days * 100\n",
    "    )\n",
    "    \n",
    "    if df.empty:\n",
    "        raise ValueError(f\"No data fetched for {symbol} - {timeframe}\")\n",
    "    \n",
    "    print(f\"  âœ“ Fetched {len(df)} candles\")\n",
    "    \n",
    "    # Validate and clean\n",
    "    print(f\"  ðŸ” Validating data quality...\")\n",
    "    df, quality_metrics = validator.validate_and_clean(df)\n",
    "    print(f\"  âœ“ Quality score: {quality_metrics.get('quality_score', 0):.2f}/100\")\n",
    "    \n",
    "    # Create features\n",
    "    print(f\"  ðŸ”§ Engineering features...\")\n",
    "    df = feature_engineer.create_features(df)\n",
    "    print(f\"  âœ“ Created {len(df.columns)} features\")\n",
    "    \n",
    "    # Create target variable\n",
    "    print(f\"  ðŸŽ¯ Creating target labels...\")\n",
    "    df = create_target(df, forward_periods=3)\n",
    "    \n",
    "    target_dist = df['target'].value_counts().to_dict()\n",
    "    print(f\"  âœ“ Target distribution: SELL={target_dist.get(0, 0)}, HOLD={target_dist.get(1, 0)}, BUY={target_dist.get(2, 0)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_target(df: pd.DataFrame, forward_periods: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"Create target variable based on future returns.\"\"\"\n",
    "    # Calculate forward returns\n",
    "    df['future_return'] = df['close'].shift(-forward_periods) / df['close'] - 1\n",
    "    \n",
    "    # Define thresholds\n",
    "    buy_threshold = 0.01   # 1% gain\n",
    "    sell_threshold = -0.01  # 1% loss\n",
    "    \n",
    "    # Create target: 0=SELL, 1=HOLD, 2=BUY\n",
    "    df['target'] = 1  # Default to HOLD\n",
    "    df.loc[df['future_return'] > buy_threshold, 'target'] = 2  # BUY\n",
    "    df.loc[df['future_return'] < sell_threshold, 'target'] = 0  # SELL\n",
    "    \n",
    "    # Drop last few rows (no future data)\n",
    "    df = df.iloc[:-forward_periods]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"âœ“ Data preparation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data for all timeframes\n",
    "datasets = {}\n",
    "\n",
    "for timeframe in tqdm(TIMEFRAMES, desc=\"Fetching data\"):\n",
    "    try:\n",
    "        df = fetch_and_prepare_data(SYMBOL, timeframe, HISTORICAL_DAYS)\n",
    "        datasets[timeframe] = df\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error fetching data for {timeframe}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nâœ“ Successfully prepared data for {len(datasets)} timeframes\")\n",
    "for tf, df in datasets.items():\n",
    "    print(f\"  {tf}: {len(df)} samples, {len(df.columns)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Model Training\n",
    "Train XGBoost models for each timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df: pd.DataFrame, train_ratio: float = 0.7, val_ratio: float = 0.15) -> Tuple:\n",
    "    \"\"\"Split data into train, validation, and test sets.\"\"\"\n",
    "    total = len(df)\n",
    "    train_end = int(total * train_ratio)\n",
    "    val_end = int(total * (train_ratio + val_ratio))\n",
    "    \n",
    "    train_df = df.iloc[:train_end]\n",
    "    val_df = df.iloc[train_end:val_end]\n",
    "    test_df = df.iloc[val_end:]\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "def prepare_for_model(df: pd.DataFrame, target_col: str = 'target') -> Tuple:\n",
    "    \"\"\"Prepare features and target for model training.\"\"\"\n",
    "    # Exclude columns\n",
    "    exclude_cols = ['timestamp', 'symbol', 'timeframe', 'open', 'high', 'low', 'close', 'volume', \n",
    "                   'future_return', target_col]\n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[target_col].copy()\n",
    "    \n",
    "    # Replace inf with nan and fill\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "    X = X.fillna(method='ffill').fillna(0)\n",
    "    \n",
    "    return X, y, feature_cols\n",
    "\n",
    "\n",
    "print(\"âœ“ Training helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "trained_models = {}\n",
    "training_results = []\n",
    "\n",
    "for timeframe in tqdm(TIMEFRAMES, desc=\"Training models\"):\n",
    "    if timeframe not in datasets:\n",
    "        print(f\"âš  Skipping {timeframe} - no data available\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ðŸ¤– Training model for {SYMBOL} - {timeframe}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    df = datasets[timeframe]\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Split data\n",
    "    train_df, val_df, test_df = split_data(df, TRAIN_RATIO, VAL_RATIO)\n",
    "    print(f\"\\nðŸ“Š Data split: Train={len(train_df)}, Val={len(val_df)}, Test={len(test_df)}\")\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X_train, y_train, feature_names = prepare_for_model(train_df, 'target')\n",
    "    X_val, y_val, _ = prepare_for_model(val_df, 'target')\n",
    "    X_test, y_test, _ = prepare_for_model(test_df, 'target')\n",
    "    \n",
    "    print(f\"âœ“ Features prepared: {len(feature_names)} features\")\n",
    "    \n",
    "    # Create and train model\n",
    "    print(f\"\\nðŸ”„ Training XGBoost model...\")\n",
    "    model = XGBoostTradingModel()\n",
    "    \n",
    "    # Store feature names\n",
    "    model.feature_names = feature_names\n",
    "    \n",
    "    # Scale features\n",
    "    model.scaler = StandardScaler()\n",
    "    X_train_scaled = model.scaler.fit_transform(X_train)\n",
    "    X_val_scaled = model.scaler.transform(X_val)\n",
    "    X_test_scaled = model.scaler.transform(X_test)\n",
    "    \n",
    "    # Create XGBoost model\n",
    "    model.model = XGBClassifier(**MODEL_PARAMS)\n",
    "    \n",
    "    # Train with validation\n",
    "    model.model.fit(\n",
    "        X_train_scaled,\n",
    "        y_train,\n",
    "        eval_set=[(X_train_scaled, y_train), (X_val_scaled, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    train_pred = model.model.predict(X_train_scaled)\n",
    "    val_pred = model.model.predict(X_val_scaled)\n",
    "    test_pred = model.model.predict(X_test_scaled)\n",
    "    \n",
    "    train_acc = (train_pred == y_train).mean()\n",
    "    val_acc = (val_pred == y_val).mean()\n",
    "    test_acc = (test_pred == y_test).mean()\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ Results:\")\n",
    "    print(f\"  Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"  Val Accuracy:   {val_acc:.4f}\")\n",
    "    print(f\"  Test Accuracy:  {test_acc:.4f}\")\n",
    "    print(f\"  Training Time:  {training_time:.2f}s\")\n",
    "    \n",
    "    # Get feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': model.model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False).head(10)\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Top 10 Features:\")\n",
    "    for idx, row in feature_importance.iterrows():\n",
    "        print(f\"  {row['feature']:<30} {row['importance']:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    model_filename = f\"models/xgboost_{SYMBOL}_{timeframe}.pkl\"\n",
    "    model.save(model_filename)\n",
    "    print(f\"\\nðŸ’¾ Model saved: {model_filename}\")\n",
    "    \n",
    "    # Store results\n",
    "    trained_models[timeframe] = model\n",
    "    training_results.append({\n",
    "        'timeframe': timeframe,\n",
    "        'train_accuracy': train_acc,\n",
    "        'val_accuracy': val_acc,\n",
    "        'test_accuracy': test_acc,\n",
    "        'training_time': training_time,\n",
    "        'num_features': len(feature_names),\n",
    "        'train_samples': len(train_df),\n",
    "        'model_file': model_filename\n",
    "    })\n",
    "\n",
    "print(f\"\\n\\n{'='*60}\")\n",
    "print(f\"âœ“ Training complete for {len(trained_models)} models\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary table\n",
    "results_df = pd.DataFrame(training_results)\n",
    "\n",
    "print(\"\\nðŸ“Š Training Summary:\")\n",
    "print(\"=\" * 100)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Calculate averages\n",
    "print(f\"\\nðŸ“ˆ Average Metrics:\")\n",
    "print(f\"  Train Accuracy: {results_df['train_accuracy'].mean():.4f}\")\n",
    "print(f\"  Val Accuracy:   {results_df['val_accuracy'].mean():.4f}\")\n",
    "print(f\"  Test Accuracy:  {results_df['test_accuracy'].mean():.4f}\")\n",
    "print(f\"  Training Time:  {results_df['training_time'].mean():.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "results_df.set_index('timeframe')[['train_accuracy', 'val_accuracy', 'test_accuracy']].plot(\n",
    "    kind='bar', ax=axes[0], rot=0\n",
    ")\n",
    "axes[0].set_title('Model Accuracy by Timeframe', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_xlabel('Timeframe')\n",
    "axes[0].legend(['Train', 'Validation', 'Test'])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Training time\n",
    "results_df.set_index('timeframe')['training_time'].plot(\n",
    "    kind='bar', ax=axes[1], color='coral', rot=0\n",
    ")\n",
    "axes[1].set_title('Training Time by Timeframe', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Time (seconds)')\n",
    "axes[1].set_xlabel('Timeframe')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/training_summary.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Visualization saved to plots/training_summary.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Model Download\n",
    "Package all trained models and download as ZIP file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all model files\n",
    "model_files = list(Path('models').glob('*.pkl'))\n",
    "\n",
    "print(f\"ðŸ“¦ Found {len(model_files)} model files:\")\n",
    "for model_file in model_files:\n",
    "    size_mb = model_file.stat().st_size / (1024 * 1024)\n",
    "    print(f\"  â€¢ {model_file.name} ({size_mb:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ZIP file with all models\n",
    "zip_filename = f'trained_models_{SYMBOL}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.zip'\n",
    "\n",
    "print(f\"\\nðŸ“¦ Creating ZIP archive: {zip_filename}\")\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    # Add model files\n",
    "    for model_file in model_files:\n",
    "        zipf.write(model_file, arcname=f\"models/{model_file.name}\")\n",
    "        print(f\"  âœ“ Added {model_file.name}\")\n",
    "    \n",
    "    # Add training summary\n",
    "    results_df.to_csv('training_summary.csv', index=False)\n",
    "    zipf.write('training_summary.csv', arcname='training_summary.csv')\n",
    "    print(f\"  âœ“ Added training_summary.csv\")\n",
    "    \n",
    "    # Add plot if exists\n",
    "    if os.path.exists('plots/training_summary.png'):\n",
    "        zipf.write('plots/training_summary.png', arcname='training_summary.png')\n",
    "        print(f\"  âœ“ Added training_summary.png\")\n",
    "\n",
    "zip_size_mb = Path(zip_filename).stat().st_size / (1024 * 1024)\n",
    "print(f\"\\nâœ“ ZIP archive created: {zip_filename} ({zip_size_mb:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the ZIP file\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    print(f\"\\nâ¬‡ï¸ Downloading {zip_filename} to your local machine...\")\n",
    "    files.download(zip_filename)\n",
    "    print(\"âœ“ Download started! Check your browser downloads.\")\n",
    "else:\n",
    "    print(f\"\\nâœ“ ZIP file ready: {zip_filename}\")\n",
    "    print(f\"  You can find it in the current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ‰ Training Complete!\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "1. **Extract the downloaded ZIP file** on your local machine\n",
    "2. **Copy the model files** to your project's `models/` directory\n",
    "3. **Update your config** to use the new model paths\n",
    "4. **Run predictions** using the trained models\n",
    "\n",
    "### Model Files:\n",
    "- `xgboost_BTCUSDT_15m.pkl` - 15-minute timeframe model\n",
    "- `xgboost_BTCUSDT_1h.pkl` - 1-hour timeframe model  \n",
    "- `xgboost_BTCUSDT_4h.pkl` - 4-hour timeframe model\n",
    "\n",
    "### Additional Files:\n",
    "- `training_summary.csv` - Training metrics and details\n",
    "- `training_summary.png` - Performance visualization\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Trading! ðŸ“ˆ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}